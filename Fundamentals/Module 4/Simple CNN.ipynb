{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81c79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Subset, random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec78609",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365822d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __inti__(self, root_dir, transform = None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"jpg\")\n",
    "\n",
    "        self.labels = os.path.join(self.root_dir, \"labels\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = self.retrieve_image(index)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "            label = self.labels(index)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def retrieve_image(self,index):\n",
    "        img_name = f\"image_{index + 1:05d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2304e17",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc312849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the neural network.\n",
    "\n",
    "        Args:\n",
    "            num_classes: The number of output classes for the final layer.\n",
    "        \"\"\"\n",
    "        # Call the constructor of the parent class (nn.Module)\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the third convolutional block\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the layer to flatten the feature maps\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Define the fully connected (dense) layers\n",
    "        # Input image is 32x32, after 3 pooling layers: 4x4\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb1130",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "185c99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_model = SimpleCNN(num_classes = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60440daa",
   "metadata": {},
   "source": [
    "## Observe Data Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc04fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_flow(model):\n",
    "    \"\"\"\n",
    "    Prints the shape of a tensor as it flows through each layer of the model.\n",
    "\n",
    "    Args:\n",
    "        model: An instance of the PyTorch model to inspect.\n",
    "    \"\"\"\n",
    "    # Create a sample input tensor (batch_size, channels, height, width)\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "    # Track the tensor shape at each stage\n",
    "    print(f\"Input shape: \\t\\t{x.shape}\")\n",
    "\n",
    "    # First conv block\n",
    "    x = model.conv1(x)\n",
    "    print(f\"After conv1: \\t\\t{x.shape}\")\n",
    "    x = model.relu1(x)\n",
    "    x = model.pool1(x)\n",
    "    print(f\"After pool1: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Second conv block\n",
    "    x = model.conv2(x)\n",
    "    print(f\"After conv2: \\t\\t{x.shape}\")\n",
    "    x = model.relu2(x)\n",
    "    x = model.pool2(x)\n",
    "    print(f\"After pool2: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Third conv block\n",
    "    x = model.conv3(x)\n",
    "    print(f\"After conv3: \\t\\t{x.shape}\")\n",
    "    x = model.relu3(x)\n",
    "    x = model.pool3(x)\n",
    "    print(f\"After pool3: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Flatten using the model's flatten layer\n",
    "    x = model.flatten(x)\n",
    "    print(f\"After flatten: \\t\\t{x.shape}\")\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = model.fc1(x)\n",
    "    print(f\"After fc1: \\t\\t{x.shape}\")\n",
    "    x = model.relu4(x)\n",
    "    x = model.dropout(x)\n",
    "    x = model.fc2(x)\n",
    "    print(f\"Output shape (fc2): \\t{x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d305ebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n",
      "\n",
      "--- Tracing Data Flow ---\n",
      "Input shape: \t\ttorch.Size([1, 3, 32, 32])\n",
      "After conv1: \t\ttorch.Size([1, 32, 32, 32])\n",
      "After pool1: \t\ttorch.Size([1, 32, 16, 16])\n",
      "After conv2: \t\ttorch.Size([1, 64, 16, 16])\n",
      "After pool2: \t\ttorch.Size([1, 64, 8, 8])\n",
      "After conv3: \t\ttorch.Size([1, 128, 8, 8])\n",
      "After pool3: \t\ttorch.Size([1, 128, 4, 4])\n",
      "After flatten: \t\ttorch.Size([1, 2048])\n",
      "After fc1: \t\ttorch.Size([1, 512])\n",
      "Output shape (fc2): \ttorch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "# Print the model's architecture\n",
    "print(prototype_model)\n",
    "\n",
    "# Call the helper function to visualize the data flow\n",
    "print(\"\\n--- Tracing Data Flow ---\")\n",
    "print_data_flow(prototype_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d4f94",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "985e5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(prototype_model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d65679",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4037cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_loader, test_loader, val_loader, loss_function, optimizer, num_epochs, device):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_losses = 0.0\n",
    "\n",
    "        for images, labels in range(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = running_loss/len(train_loader.dataset)\n",
    "        # Using len(train_loader would output the number of batches, not the number of individual images)\n",
    "\n",
    "        training_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        running_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                val_loss = loss_function(outputs, labels)\n",
    "\n",
    "                running_val_loss += (val_loss.item() * images.size(0))\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "\n",
    "                correct += (predicted == labels).sum.item()\n",
    "\n",
    "            \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        epoch_accuracy = correct / total * 100\n",
    "\n",
    "        val_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    print('---Finished Training---')\n",
    "\n",
    "    metrics = [training_losses, val_losses, val_accuracies]\n",
    "\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
